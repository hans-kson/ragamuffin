{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<imports>\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "#<\\imports>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<get hf token>\n",
    "HF_TOKEN = getpass(\"Huggingface Token : \")\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_TOKEN\n",
    "#<\\get hf token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<load document>\n",
    "loader = PyPDFLoader(\"data/sample_data.pdf\")\n",
    "data = loader.load()\n",
    "#<\\load document>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<make chunks>\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 512,chunk_overlap = 100)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "#<\\make chunks>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<embeddings and chroma db>\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(api_key = HF_TOKEN,model_name = \"thenlper/gte-large\")\n",
    "\n",
    "db = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "#<embeddings and chroma db>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<regular vector search retriever>\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "#above retriever will retrieve 4 documents based on your query. \n",
    "#But which among them is the most relevant? in order to rank that we use reranking.\n",
    "\n",
    "#<\\regular vector search retriever>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<cross encoders>\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "re_rank_retriever = ContextualCompressionRetriever(base_compressor=compressor, \n",
    "                                                   base_retriever=retriever\n",
    "                                                )\n",
    "#<\\cross encoders>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<llm>\n",
    "model = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "                       model_kwargs = {\"temperature\":0.5,\n",
    "                                       \"max_new_tokens\":512,\n",
    "                                       \"max_length\":64,\n",
    "                                       \"return_full_text\":False\n",
    "                                    }\n",
    "                    )\n",
    "#<\\llm>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a dream?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<template>\n",
    "template = \"\"\"\n",
    "<|system|>\n",
    "You are an AI Assistant that follows instructions extremely well.\n",
    "Please be truthful and give direct answers. Please tell 'I don't know' if user query is not in CONTEXT\n",
    "\n",
    "CONTEXT: {context}\n",
    "</s>\n",
    "<|user|>\n",
    "{query}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "#<\\template>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<prompt>\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "#<\\prompt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<rag pipe>\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    {\"context\": re_rank_retriever, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "#<\\rag pipe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<test rag>\n",
    "query = \"What is a dream?\"\n",
    "response = chain.invoke(query)\n",
    "print(response)\n",
    "#<\\test rag>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragamuffin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
