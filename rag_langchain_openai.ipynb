{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<imports>\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "#<\\imports>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<set openai api token>\n",
    "#url for how to use azure open ai: https://python.langchain.com/v0.2/docs/integrations/llms/azure_openai/\n",
    "\n",
    "OPENAI_KEY = getpass()\n",
    "# Set the API token in the environment variable\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
    "#<\\set openai api token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<helper functions>\n",
    "def http_get(url:str, path:str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a URL to a given path on disc\n",
    "    \"\"\"\n",
    "    if os.path.dirname(path) != \"\":\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    req = requests.get(url, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Exception when trying to download {}. Response {}\".format(url, req.status_code), file=sys.stderr)\n",
    "        req.raise_for_status()\n",
    "        return\n",
    "\n",
    "    download_filepath = path + \"_part\"\n",
    "    with open(download_filepath, \"wb\") as file_binary:\n",
    "        content_length = req.headers.get(\"Content-Length\")\n",
    "        total = int(content_length) if content_length is not None else None\n",
    "        progress = tqdm(unit=\"B\", total=total, unit_scale=True)\n",
    "        for chunk in req.iter_content(chunk_size=1024):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                progress.update(len(chunk))\n",
    "                file_binary.write(chunk)\n",
    "\n",
    "    os.rename(download_filepath, path)\n",
    "    progress.close()\n",
    "#<\\helper functions>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<load wikipedia data>\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "#<\\load wikipedia data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<check data>\n",
    "wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        #Add all paragraphs\n",
    "        #passages.extend(data['paragraphs'])\n",
    "\n",
    "        #Only add the first paragraph\n",
    "        passages.append(data['paragraphs'][0])\n",
    "\n",
    "print(\"Passages:\", len(passages))\n",
    "#<\\check data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<use subset of wiki>\n",
    "passages = [passage for passage in passages for x in ['fish', 'india', 'cheetah']\n",
    "              if x in passage.lower().split()]\n",
    "passages = [passage for passage in passages for x in ['flying fish', 'india', 'cheetah']\n",
    "              if x in passage.lower()]\n",
    "\n",
    "print(len(passages))\n",
    "print(passages[0])\n",
    "#<\\use subset of wiki>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<load llm>\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "#<\\load llm>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<make embedings and store in chroma vec db>\n",
    "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "docs = [Document(page_content=doc) for doc in passages]\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "\n",
    "#create vector DB of docs and embeddings\n",
    "chroma_db = Chroma.from_documents(documents=chunked_docs, collection_name='wiki_db',\n",
    "                                  embedding=openai_embed_model,\n",
    "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
    "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
    "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "                                  persist_directory=\"./wiki_db\")\n",
    "\n",
    "#<\\make embedings and store in chroma vec db>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<load chroma db from disc if already created>\n",
    "chroma_db = Chroma(persist_directory=\"./wiki_db\",\n",
    "                   collection_name='wiki_db',\n",
    "                   embedding_function=openai_embed_model)\n",
    "#<\\load chroma db from disc if already created>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<make retriever>\n",
    "similarity_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                                              search_kwargs={\"k\": 5, \"score_threshold\": 0.2})\n",
    "#<\\make retriever>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<test retriever>\n",
    "similarity_retriever.invoke('what is the capital of India?')\n",
    "#<\\test retriever>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<make QnA rag chain>\n",
    "# pprompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            Keep the answer upto 5 lines unless the user asks for more information\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Answer:\n",
    "         \"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "#helper\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#rag chain\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": (similarity_retriever\n",
    "                      |\n",
    "                    format_docs),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "      |\n",
    "    prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    ")\n",
    "#<\\make QnA rag chain>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<test query>\n",
    "query = \"What is the fastest fish to eat?\"\n",
    "result = qa_rag_chain.invoke(query)\n",
    "print(result.content)\n",
    "#<\\test query>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<make conversational rag>\n",
    "# rephrase_prompt = hub.pull(\"langchain-ai/chat-langchain-rephrase\")\n",
    "# rephrase_prompt\n",
    "rephrase_system_prompt = \"\"\"Given a chat history and the latest user question\n",
    "which might reference context in the chat history, formulate a standalone question\n",
    "which can be understood without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "rephrase_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rephrase_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#history aware retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatgpt, similarity_retriever, rephrase_prompt\n",
    ")\n",
    "# print(history_aware_retriever)\n",
    "\n",
    "#qa rag chain\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "                      Use the following pieces of retrieved context to answer the question.\n",
    "                      If you don't know the answer, just say that you don't know.\n",
    "                      Keep the answer upto 5 lines unless the user asks for more information\n",
    "\n",
    "                      Context:\n",
    "                      {context}\n",
    "                  \"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chatgpt, qa_prompt)\n",
    "\n",
    "qa_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "# print(qa_rag_chain)\n",
    "\n",
    "#<\\make conversational rag>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is the capital of India?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in qa_rag_chain.stream({\"input\": question, \"chat_history\": chat_history}):\n",
    "  print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me more about this city\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can fish really fly?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the fastest animal?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about its different species\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<example with returning the source of info>\n",
    "chat_history = []\n",
    "question = \"which is the fastest animal?\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print('Answer:', response['answer'])\n",
    "print('Sources:')\n",
    "for document in response['context']:\n",
    "    print(document)\n",
    "    print()\n",
    "#<\\example with returning the source of info>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=question),\n",
    "                     AIMessage(content=response[\"answer\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me more, including different types of this animal and their details\"\n",
    "response = qa_rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print('Answer:', response['answer'])\n",
    "print('Sources:')\n",
    "for document in response['context']:\n",
    "    print(document)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
